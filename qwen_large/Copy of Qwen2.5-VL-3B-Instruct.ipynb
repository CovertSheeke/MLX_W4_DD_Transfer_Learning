{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"machine_shape":"hm","gpuType":"L4","provenance":[{"file_id":"https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct.ipynb","timestamp":1751300482595}]},"accelerator":"GPU","kaggle":{"accelerator":"gpu"},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","source":["!pip install -U transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qOyuujzRetju","executionInfo":{"status":"ok","timestamp":1751298635742,"user_tz":-60,"elapsed":4503,"user":{"displayName":"","userId":""}},"outputId":"52115196-7b68-42f7-a3c4-811180933cac"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n"]}]},{"cell_type":"markdown","source":["Model page: https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct\n","\n","âš ï¸ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)\n","\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) ðŸ™"],"metadata":{"id":"29RbpPwMetjw"}},{"cell_type":"code","source":["# Use a pipeline as a high-level helper\n","from transformers import pipeline\n","\n","pipe = pipeline(\"image-text-to-text\", model=\"Qwen/Qwen2.5-VL-3B-Instruct\")\n","\n","messages = [\n","    {\n","        \"role\": \"user\",\n","        \"content\": [\n","            {\n","                \"type\": \"image\",\n","                \"image\": \"./IMG_4864.jpg\",\n","            },\n","            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n","        ],\n","    }\n","]\n","pipe(text=messages, max_new_tokens = 128)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ec0Hv9wOetjy","executionInfo":{"status":"ok","timestamp":1751299408175,"user_tz":-60,"elapsed":61731,"user":{"displayName":"","userId":""}},"outputId":"78c2e2c7-596b-45d0-fb29-3732e6629e6c"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'input_text': [{'role': 'user',\n","    'content': [{'type': 'image', 'image': './IMG_4864.jpg'},\n","     {'type': 'text', 'text': 'Describe this image.'}]}],\n","  'generated_text': [{'role': 'user',\n","    'content': [{'type': 'image', 'image': './IMG_4864.jpg'},\n","     {'type': 'text', 'text': 'Describe this image.'}]},\n","   {'role': 'assistant',\n","    'content': 'The image depicts two individuals in an indoor setting, likely an office or classroom environment. The background features a whiteboard with various diagrams and text written on it. The whiteboard is mounted on a wall that has a window with blinds partially drawn, allowing some natural light to enter the room.\\n\\n### Detailed Description:\\n\\n1. **Individuals:**\\n   - There are two people in the image.\\n   - One person is standing and appears to be explaining something, holding a marker in their right hand.\\n   - The other person is seated and smiling, seemingly engaged in the discussion or presentation.\\n\\n2. **Whiteboard:**\\n   - The'}]}]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["messages = [\n","    {\n","        \"role\": \"user\",\n","        \"content\": [\n","            {\n","                \"type\": \"image\",\n","                \"image\": \"./Bes.jpg\",\n","            },\n","            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n","        ],\n","    }\n","]\n","pipe(text=messages)"],"metadata":{"id":"MBpGW7Uplj6T","executionInfo":{"status":"ok","timestamp":1751299881213,"user_tz":-60,"elapsed":29979,"user":{"displayName":"","userId":""}},"outputId":"eaa36246-58d8-4f6b-f057-7adfdb40c92b","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'input_text': [{'role': 'user',\n","    'content': [{'type': 'image', 'image': './Bes.jpg'},\n","     {'type': 'text', 'text': 'Describe this image.'}]}],\n","  'generated_text': [{'role': 'user',\n","    'content': [{'type': 'image', 'image': './Bes.jpg'},\n","     {'type': 'text', 'text': 'Describe this image.'}]},\n","   {'role': 'assistant',\n","    'content': 'The image shows a person sitting at a desk in front of a computer monitor. The individual is giving'}]}]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["help(pipe)"],"metadata":{"id":"ZiXUhQmim1Ao","executionInfo":{"status":"ok","timestamp":1751299881601,"user_tz":-60,"elapsed":75,"user":{"displayName":"","userId":""}},"outputId":"b4140d8f-e482-4e1f-ec38-05baf7ac525b","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on ImageTextToTextPipeline in module transformers.pipelines.image_text_to_text object:\n","\n","class ImageTextToTextPipeline(transformers.pipelines.base.Pipeline)\n"," |  ImageTextToTextPipeline(*args, **kwargs)\n"," |  \n"," |  Image-text-to-text pipeline using an `AutoModelForImageTextToText`. This pipeline generates text given an image and text.\n"," |  When the underlying model is a conversational model, it can also accept one or more chats,\n"," |  in which case the pipeline will operate in chat mode and will continue the chat(s) by adding its response(s).\n"," |  Each chat takes the form of a list of dicts, where each dict contains \"role\" and \"content\" keys.\n"," |  \n"," |  Unless the model you're using explicitly sets these generation parameters in its configuration files\n"," |  (`generation_config.json`), the following default values will be used:\n"," |  - max_new_tokens: 256\n"," |  \n"," |  Example:\n"," |  \n"," |  ```python\n"," |  >>> from transformers import pipeline\n"," |  \n"," |  >>> pipe = pipeline(task=\"image-text-to-text\", model=\"Salesforce/blip-image-captioning-base\")\n"," |  >>> pipe(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\", text=\"A photo of\")\n"," |  [{'generated_text': 'a photo of two birds'}]\n"," |  ```\n"," |  \n"," |  ```python\n"," |  >>> from transformers import pipeline\n"," |  \n"," |  >>> pipe = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-interleave-qwen-0.5b-hf\")\n"," |  >>> messages = [\n"," |  >>>     {\n"," |  >>>         \"role\": \"user\",\n"," |  >>>         \"content\": [\n"," |  >>>             {\n"," |  >>>                 \"type\": \"image\",\n"," |  >>>                 \"url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n"," |  >>>             },\n"," |  >>>             {\"type\": \"text\", \"text\": \"Describe this image.\"},\n"," |  >>>         ],\n"," |  >>>     },\n"," |  >>>     {\n"," |  >>>         \"role\": \"assistant\",\n"," |  >>>         \"content\": [\n"," |  >>>             {\"type\": \"text\", \"text\": \"There is a dog and\"},\n"," |  >>>         ],\n"," |  >>>     },\n"," |  >>> ]\n"," |  >>> pipe(text=messages, max_new_tokens=20, return_full_text=False)\n"," |  [{'input_text': [{'role': 'user',\n"," |      'content': [{'type': 'image',\n"," |      'url': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'},\n"," |      {'type': 'text', 'text': 'Describe this image.'}]},\n"," |  {'role': 'assistant',\n"," |      'content': [{'type': 'text', 'text': 'There is a dog and'}]}],\n"," |  'generated_text': ' a person in the image. The dog is sitting on the sand, and the person is sitting on'}]\n"," |  ```\n"," |  \n"," |  Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\n"," |  \n"," |  This image-text to text pipeline can currently be loaded from pipeline() using the following task identifier:\n"," |  \"image-text-to-text\".\n"," |  \n"," |  See the list of available models on\n"," |  [huggingface.co/models](https://huggingface.co/models?pipeline_tag=image-text-to-text).\n"," |  \n"," |  Arguments:\n"," |      model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n"," |          The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n"," |          [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n"," |      processor ([`ProcessorMixin`]):\n"," |          The processor that will be used by the pipeline to encode data for the model. This object inherits from\n"," |          [`ProcessorMixin`]. Processor is a composite object that might contain `tokenizer`, `feature_extractor`, and\n"," |          `image_processor`.\n"," |      modelcard (`str` or [`ModelCard`], *optional*):\n"," |          Model card attributed to the model for this pipeline.\n"," |      framework (`str`, *optional*):\n"," |          The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n"," |          installed.\n"," |  \n"," |          If no framework is specified, will default to the one currently installed. If no framework is specified and\n"," |          both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n"," |          provided.\n"," |      task (`str`, defaults to `\"\"`):\n"," |          A task-identifier for the pipeline.\n"," |      num_workers (`int`, *optional*, defaults to 8):\n"," |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n"," |          workers to be used.\n"," |      batch_size (`int`, *optional*, defaults to 1):\n"," |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n"," |          the batch to use, for inference this is not always beneficial, please read [Batching with\n"," |          pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n"," |      args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n"," |          Reference to the object in charge of parsing supplied pipeline parameters.\n"," |      device (`int`, *optional*, defaults to -1):\n"," |          Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n"," |          the associated CUDA device id. You can pass native `torch.device` or a `str` too\n"," |      torch_dtype (`str` or `torch.dtype`, *optional*):\n"," |          Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n"," |          (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`)\n"," |      binary_output (`bool`, *optional*, defaults to `False`):\n"," |          Flag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\n"," |          the raw output data e.g. text.\n"," |  \n"," |  Method resolution order:\n"," |      ImageTextToTextPipeline\n"," |      transformers.pipelines.base.Pipeline\n"," |      transformers.pipelines.base._ScikitCompat\n"," |      abc.ABC\n"," |      transformers.utils.hub.PushToHubMixin\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __call__(self, images: Union[str, list[str], list[list[str]], ForwardRef('Image.Image'), list['Image.Image'], list[list['Image.Image']], list[dict], NoneType] = None, text: Union[str, list[str], list[dict], NoneType] = None, **kwargs) -> Union[list[dict[str, Any]], list[list[dict[str, Any]]]]\n"," |      Generate a text given text and the image(s) passed as inputs.\n"," |      \n"," |      Args:\n"," |          images (`str`, `list[str]`, `PIL.Image, `list[PIL.Image]`, `list[dict[str, Union[str, PIL.Image]]]`):\n"," |              The pipeline handles three types of images:\n"," |      \n"," |              - A string containing a HTTP(s) link pointing to an image\n"," |              - A string containing a local path to an image\n"," |              - An image loaded in PIL directly\n"," |      \n"," |              The pipeline accepts either a single image or a batch of images. Finally, this pipeline also supports\n"," |              the chat format (see `text`) containing images and text in this argument.\n"," |          text (str, list[str], `list[dict[str, Union[str, PIL.Image]]]`):\n"," |              The text to be used for generation. If a list of strings is passed, the length of the list should be\n"," |              the same as the number of images. Text can also follow the chat format: a list of dictionaries where\n"," |              each dictionary represents a message in a conversation. Each dictionary should have two keys: 'role'\n"," |              and 'content'. 'role' should be one of 'user', 'system' or 'assistant'. 'content' should be a list of\n"," |              dictionary containing the text of the message and the type of the message. The type of the message\n"," |              can be either 'text' or 'image'. If the type is 'image', no text is needed.\n"," |          return_tensors (`bool`, *optional*, defaults to `False`):\n"," |              Returns the tensors of predictions (as token indices) in the outputs. If set to\n"," |              `True`, the decoded text is not returned.\n"," |          return_text (`bool`, *optional*):\n"," |              Returns the decoded texts in the outputs.\n"," |          return_full_text (`bool`, *optional*, defaults to `True`):\n"," |              If set to `False` only added text is returned, otherwise the full text is returned. Cannot be\n"," |              specified at the same time as `return_text`.\n"," |          clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n"," |              Whether or not to clean up the potential extra spaces in the text output.\n"," |          continue_final_message( `bool`, *optional*): This indicates that you want the model to continue the\n"," |              last message in the input chat rather than starting a new one, allowing you to \"prefill\" its response.\n"," |              By default this is `True` when the final message in the input chat has the `assistant` role and\n"," |              `False` otherwise, but you can manually override that behaviour by setting this flag.\n"," |      \n"," |      Return:\n"," |          A list or a list of list of `dict`: Each result comes as a dictionary with the following key (cannot\n"," |          return a combination of both `generated_text` and `generated_token_ids`):\n"," |      \n"," |          - **generated_text** (`str`, present when `return_text=True`) -- The generated text.\n"," |          - **generated_token_ids** (`torch.Tensor`, present when `return_tensors=True`) -- The token\n"," |              ids of the generated text.\n"," |          - **input_text** (`str`) -- The input text.\n"," |  \n"," |  __init__(self, *args, **kwargs)\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |  \n"," |  postprocess(self, model_outputs, return_type=<ReturnType.FULL_TEXT: 2>, continue_final_message=None, **postprocess_kwargs)\n"," |      Postprocess will receive the raw outputs of the `_forward` method, generally tensors, and reformat them into\n"," |      something more friendly. Generally it will output a list or a dict or results (containing just strings and\n"," |      numbers).\n"," |  \n"," |  preprocess(self, inputs=None, timeout=None, continue_final_message=None, **processing_kwargs)\n"," |      Preprocess will take the `input_` of a specific pipeline and return a dictionary of everything necessary for\n"," |      `_forward` to run properly. It should contain at least one tensor, but might have arbitrary other items.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data and other attributes defined here:\n"," |  \n"," |  __abstractmethods__ = frozenset()\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from transformers.pipelines.base.Pipeline:\n"," |  \n"," |  check_model_type(self, supported_models: Union[list[str], dict])\n"," |      Check if the model class is in supported by the pipeline.\n"," |      \n"," |      Args:\n"," |          supported_models (`list[str]` or `dict`):\n"," |              The list of models supported by the pipeline, or a dictionary with model class values.\n"," |  \n"," |  device_placement(self)\n"," |      Context Manager allowing tensor allocation on the user-specified device in framework agnostic way.\n"," |      \n"," |      Returns:\n"," |          Context manager\n"," |      \n"," |      Examples:\n"," |      \n"," |      ```python\n"," |      # Explicitly ask for tensor allocation on CUDA device :0\n"," |      pipe = pipeline(..., device=0)\n"," |      with pipe.device_placement():\n"," |          # Every framework specific tensor allocation will be done on the request device\n"," |          output = pipe(...)\n"," |      ```\n"," |  \n"," |  ensure_tensor_on_device(self, **inputs)\n"," |      Ensure PyTorch tensors are on the specified device.\n"," |      \n"," |      Args:\n"," |          inputs (keyword arguments that should be `torch.Tensor`, the rest is ignored):\n"," |              The tensors to place on `self.device`.\n"," |          Recursive on lists **only**.\n"," |      \n"," |      Return:\n"," |          `dict[str, torch.Tensor]`: The same as `inputs` but on the proper device.\n"," |  \n"," |  forward(self, model_inputs, **forward_params)\n"," |  \n"," |  get_inference_context(self)\n"," |  \n"," |  get_iterator(self, inputs, num_workers: int, batch_size: int, preprocess_params, forward_params, postprocess_params)\n"," |  \n"," |  iterate(self, inputs, preprocess_params, forward_params, postprocess_params)\n"," |  \n"," |  predict(self, X)\n"," |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n"," |  \n"," |  push_to_hub(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str from transformers.utils.hub.PushToHubMixin\n"," |      Upload the pipeline file to the ðŸ¤— Model Hub.\n"," |      \n"," |      Parameters:\n"," |          repo_id (`str`):\n"," |              The name of the repository you want to push your pipe to. It should contain your organization name\n"," |              when pushing to a given organization.\n"," |          use_temp_dir (`bool`, *optional*):\n"," |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n"," |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n"," |          commit_message (`str`, *optional*):\n"," |              Message to commit while pushing. Will default to `\"Upload pipe\"`.\n"," |          private (`bool`, *optional*):\n"," |              Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n"," |          token (`bool` or `str`, *optional*):\n"," |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n"," |              when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n"," |              is not specified.\n"," |          max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n"," |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n"," |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n"," |              by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n"," |              Google Colab instances without any CPU OOM issues.\n"," |          create_pr (`bool`, *optional*, defaults to `False`):\n"," |              Whether or not to create a PR with the uploaded files or directly commit.\n"," |          safe_serialization (`bool`, *optional*, defaults to `True`):\n"," |              Whether or not to convert the model weights in safetensors format for safer serialization.\n"," |          revision (`str`, *optional*):\n"," |              Branch to push the uploaded files to.\n"," |          commit_description (`str`, *optional*):\n"," |              The description of the commit that will be created\n"," |          tags (`list[str]`, *optional*):\n"," |              List of tags to push on the Hub.\n"," |      \n"," |      Examples:\n"," |      \n"," |      ```python\n"," |      from transformers import pipeline\n"," |      \n"," |      pipe = pipeline(\"google-bert/bert-base-cased\")\n"," |      \n"," |      # Push the pipe to your namespace with the name \"my-finetuned-bert\".\n"," |      pipe.push_to_hub(\"my-finetuned-bert\")\n"," |      \n"," |      # Push the pipe to an organization with the name \"my-finetuned-bert\".\n"," |      pipe.push_to_hub(\"huggingface/my-finetuned-bert\")\n"," |      ```\n"," |  \n"," |  run_multi(self, inputs, preprocess_params, forward_params, postprocess_params)\n"," |  \n"," |  run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n"," |  \n"," |  save_pretrained(self, save_directory: Union[str, os.PathLike], safe_serialization: bool = True, **kwargs)\n"," |      Save the pipeline's model and tokenizer.\n"," |      \n"," |      Args:\n"," |          save_directory (`str` or `os.PathLike`):\n"," |              A path to the directory where to saved. It will be created if it doesn't exist.\n"," |          safe_serialization (`str`):\n"," |              Whether to save the model using `safetensors` or the traditional way for PyTorch or Tensorflow.\n"," |          kwargs (`dict[str, Any]`, *optional*):\n"," |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n"," |  \n"," |  transform(self, X)\n"," |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Readonly properties inherited from transformers.pipelines.base.Pipeline:\n"," |  \n"," |  torch_dtype\n"," |      Torch dtype of the model (if it's Pytorch model), `None` otherwise.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data and other attributes inherited from transformers.pipelines.base.Pipeline:\n"," |  \n"," |  default_input_names = None\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from transformers.pipelines.base._ScikitCompat:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object\n","\n"]}]},{"cell_type":"code","source":["# Load model directly\n","from transformers import AutoProcessor, AutoModelForImageTextToText\n","\n","#processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n","#model = AutoModelForImageTextToText.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")"],"metadata":{"id":"ECEg1B_Zetjz","executionInfo":{"status":"ok","timestamp":1751298708216,"user_tz":-60,"elapsed":26,"user":{"displayName":"","userId":""}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["message = [{'role': 'user',\n","    'content': [{'type': 'image', 'image': './IMG_4864.jpg'},\n","     {'type': 'text', 'text': 'Describe this image.'}]},\n","   {'role': 'assistant',\n","    'content': 'The image depicts two individuals in an indoor setting, likely an office or classroom environment. The background features'}]\n","\n","# Corrected message format\n","corrected_message = [\n","    {\n","        \"role\": \"user\",\n","        \"content\": [\n","            {\"type\": \"image\", \"image\": \"./IMG_4864.jpg\"},\n","            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n","        ],\n","    }\n","]\n","\n","# The pipe function in this case expects the input in a list of dictionaries, where each dictionary represent a turn\n","# in the conversation. For example:\n","# pipe(text=[{\"role\": \"user\", \"content\": \"Describe this image.\"}])\n","# pipe(text=[{\"role\": \"user\", \"content\": \"Describe this image.\"}, {\"role\": \"assistant\", \"content\": \"The image depicts...\"}])\n","# In this case, the error comes because the list of dictionaries contains the whole conversation, so we have to pass\n","# it to the pipe function as a list with one element, which is the list of dictionaries itself.\n","\n","pipe(text=[message], max_new_tokens=500)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"BCQWSXF0hqGg","executionInfo":{"status":"error","timestamp":1751298985413,"user_tz":-60,"elapsed":425,"user":{"displayName":"","userId":""}},"outputId":"0793a854-ee7a-43e5-cc88-075a755482e9"},"execution_count":7,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"string indices must be integers, not 'str'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-7-1214000191.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m    {'role': 'assistant',\n\u001b[1;32m      5\u001b[0m     'content': 'The image depicts two individuals in an indoor setting, likely an office or classroom environment. The background features'}]\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/image_text_to_text.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, images, text, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;31m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mChat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m             )\n\u001b[1;32m   1463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1464\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1470\u001b[0;31m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1471\u001b[0m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1472\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/image_text_to_text.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, inputs, timeout, continue_final_message, **processing_kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinue_final_message\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0mcontinue_final_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             model_inputs = self.processor.apply_chat_template(\n\u001b[0m\u001b[1;32m    389\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0madd_generation_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mcontinue_final_message\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/processing_utils.py\u001b[0m in \u001b[0;36mapply_chat_template\u001b[0;34m(self, conversation, chat_template, **kwargs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m                 \u001b[0mvideo_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m                     \u001b[0mvisuals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcontent\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"video\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m                     audio_fnames = [\n\u001b[1;32m   1570\u001b[0m                         \u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/processing_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1566\u001b[0m                 \u001b[0mvideo_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m                     \u001b[0mvisuals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcontent\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"video\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m                     audio_fnames = [\n\u001b[1;32m   1570\u001b[0m                         \u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"nyqF2H0RhihC"}}]}